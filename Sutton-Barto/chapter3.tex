\section{Finite Markov Decision Processes}

MDP Formulation: $ s_t, a_t \to r_{t+1}, s_{t+1} $

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{img/mdp_setting.png}
    \caption{MDP Formulation.}
    \label{fig:mdp}
\end{figure}

$p(s', r | s, a)$ is the probability of transitioning to state $s'$ and receiving reward $r$
given that we are in state $s$ and take action $a$. The current state $s_t$ includes all the
information about the past. (i.e., the Markov property)

The state transition probability is given by:
 \begin{equation}
    \begin{split}
        p(s'|s, a) = \sum_{r} p(s', r | s, a) \\
        \label{eq:mdp-transition-probability}
    \end{split}
 \end{equation}

The reward function is given by:
 \begin{equation}
    \begin{split}
        r(s, a) = \mathbb{E}[R_{t+1} | S_t = s, A_t = a] \\
        = \sum_{r} r \sum_{s'} p(s',r|s, a) \\
        \label{eq:mdp-reward-function}
    \end{split}
 \end{equation}

The MDP is defined by the tuple $(S, A, p, r, \gamma)$, where $S$ is the state space, $A$ is the
action space, $p$ is the state transition probability, $r$ is the reward function, and $\gamma$
is the discount factor.

Goal: Maximize expected (discounted) return over time, with discount factor $\gamma \in [0, 1]$.


\begin{equation} 
    \begin{split}
        G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots \\
        \label{eq:mdp-return}
        G_t = R_{t+1} + \gamma G_{t+1} \quad \textit{(recursive definition)} \\ 
        = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \quad \textit{(infinite sum)} \\
    \end{split}
 \end{equation}

\subsection{Policy and Value Functions}
A policy $\pi(a|s)$ is a mapping from states to actions indicating the probability of taking action $a$ in state $s$.

The \textbf{state-value function} $v_\pi(s)$ is the expected return starting from state $s$ and following policy $\pi$.

\begin{equation}
    \begin{split}
        v_\pi(s) = \mathbb{E}[G_t | S_t = s] \quad \text{for all } s \in S \\
        \label{eq:mdp-value-function}
    \end{split}
 \end{equation}

The \textbf{action-value function} $q_\pi(s, a)$ is the expected return starting from state $s$, taking action $a$, and following policy $\pi$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{img/backup_diagram1.png}
    \caption{General setting of MDP.}
    \label{fig:mdp-back-up-diagram}
 \end{figure}

\begin{equation}
    \begin{split}
        q_\pi(s, a) = \mathbb{E}[G_t | S_t = s, A_t = a] \\
        \label{eq:mdp-action-value-function}
    \end{split}
 \end{equation}

 Expected reward at time $t$ is given by:
 \begin{equation}
    \begin{split}
        \mathbb{E}[R_{t+1} | S_t = s, A_t = a] = \sum_{r} r \sum_{s'} p(s',r|s, a) \\
        = \sum_{a} \pi(a|s) \sum_{s',r} r . p(s',r|s, a) \\
        \label{eq:mdp-expected-reward}
    \end{split}
 \end{equation}

 \begin{equation}
    \begin{split}
        v_\pi(s) = \mathbb{E}[G_t | S_t = s]  \\
         = \sum_{G_t} G_t . p(G_t | S_t = s) \\
         = \sum_{G_t} G_t . p(R_t | S_t = s) \\
         = \sum_{G_t} G_t . \sum_{a} \pi(a|s) \sum_{s'} p(s'|s, a) \\
         = \sum_{G_t} G_t . \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s, a) \\
         v_\pi(s)  = \sum_{a} \pi(a|s) q_\pi(s, a) \\
        \label{eq:mdp-bellman-equation}
    \end{split}
 \end{equation}

 The value of state $s$ under policy $\pi$ is the expected return when starting from state $s$, 
 taking action $a \sim \pi(a|s)$, and then following $\pi$ forever.

 \begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{img/backup_diagram2.png}
    \caption{Back-up diagram for the value function.}
    \label{fig:mdp-back-up-diagram}
 \end{figure}


 Furthermore, the value function can be decomposed into the immediate reward plus the discounted value of the next state:
 \begin{equation}
    \begin{split}
        v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\
        v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s, a) [r + \gamma v_\pi(s')] \\
        \label{eq:mdp-bellman-equation-2}
    \end{split}
 \end{equation}

 This is the \textbf{Bellman equation} for the value function $v_\pi$, averaging over all possible next actions, weighted
 by the probability of occuring.

The \textbf{Bellman equation} for the action-value function $q_\pi(s, a)$ is given by:

  \begin{equation}
    \begin{split}
       q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
       q_\pi(s, a) = \sum_{s',r} p(s',r|s, a) [r + \gamma v_\pi(s')] \\
       \label{eq:mdp-bellman-equation-3}
    \end{split}
 \end{equation}

 Derivation below:
 \begin{equation}
    \begin{split}
        q_\pi(s, a) = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
        = \mathbb{E}[R_{t+1} | S_t = s, A_t = a]  + \gamma \mathbb{E}[G_{t+1} | S_t = s, A_t = a] \\
        = \sum_{r,s'} r . p(s',r|s, a) + \gamma \sum_{s'} v_\pi(s') p(s'|s, a) \\
        = \sum_{s',r} p(s',r|s, a) [r + \gamma v_\pi(s')] \\
        \label{eq:mdp-bellman-equation-4}
    \end{split}
 \end{equation}

 In summary,

 \begin{equation}
    \begin{split}
        v_\pi(s) = \sum_{a} \pi(a|s) q_\pi(s, a) \\
        v_\pi(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s',r|s, a) [r + \gamma v_\pi(s')] \\
        q_\pi(s, a) = \sum_{s',r} p(s',r|s, a) [r + \gamma v_\pi(s')] \\
        q_\pi(s, a) = \sum_{s',r} p(s',r|s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s', a')] \\
        \label{eq:mdp-bellman-equation-5}
    \end{split}
 \end{equation}


 \subsection{Optimal Policy and Value Functions}

 There exists an optimal policy $\pi_*$ that maximizes the value function for all states:\\
 $\pi_* \geq \pi$ for all policies $\pi$ iff $v_{\pi_{*}}(s) \geq v_\pi(s)$ for all states $s$
 and all policies $\pi$.

 The optimal value function $v_*$ is the maximum value function over all policies:

 \begin{equation}
    \begin{split}
        v_*(s) = \max_{\pi} v_\pi(s) \\
        q_*(s, a) = \max_{\pi} q_\pi(s, a) \\
        q_*(s, a) = \mathbb{E}[R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
        \label{eq:mdp-optimal-value-function}
    \end{split}
 \end{equation}

 \begin{equation}
    \begin{split}
        v_*(s) = \max_{a} q_*(s, a) \\
        \label{eq:mdp-optimal-value-function_reduced}
    \end{split}
 \end{equation}

Derivation below:

The optimal policy is deterministic. In a finite MDP, there always exists a deterministic
optimal policy. The best possible value you can get from state $s$
is by taking the best single action, then always acting optimally. i.e.,\\
the  \textit{value} of state $s$ under the optimal policy is the maximum of the \textit{expected return} for the best
action from state $s$.

\begin{equation}
    \begin{split}
        \pi_*(a|s) = \begin{cases}
            1 & \text{if,} \quad a = \arg \max_{a'} q_*(s, a') \\
            0 & \text{otherwise}
        \end{cases}
    \end{split}
\end{equation}

 \begin{equation}
    \begin{split}
        v_*(s) = \sum_{a} \pi_*(a|s) q_{\pi_*}(s, a) \\
        \implies v_*(s) = \max_{a} q_*(s, a) \\
        \label{eq:mdp-optimal-value-function-2}
    \end{split}
 \end{equation}

You're picking the greedy action with respect to $q_*(s, a)$, and then continuing optimally
 -  which is exactly what an optimal policy does.\\\\
If the value of a state is computed as an expectation over all actions, then why not use a
stochastic policy?

Suppose you have two actions, $A$ and $B$, and the value of state $s$ is computed as
 an expectation over all actions:

 Action $A$ gives you $10$, and action $B$ gives you $5$.

 If you randomly pick between them (say, 50â€“50), your expected reward is:

 \begin{equation}
    \begin{split}
        \mathbb{E}[R_{t+1} | S_t = s] = 0.5 \cdot 10 + 0.5 \cdot 5 = 7.5 \\
    \end{split}
 \end{equation}

 But if you're allowed to just always pick the better one, why wouldn't you? It always
 gives you the best possible value. It is in the best interest to be greedy.

 During policy evaluation, you're computing the value of the theoretical average:
 what happens in expectation when the agent randomly samples an action from 
 $\pi(a|s)$.\\


\textbf{Summary:}
\begin{equation}
    \begin{split}
        v_*(s) = \max_{a} q_*(s, a) \\
        v_*(s) = \max_{a} \sum_{s',r} p(s',r|s, a) [r + \gamma v_*(s')] \\
        q_*(s, a) = \sum_{s',r} p(s',r|s, a) [r + \gamma \max_{a'} q_*(s', a')] \\
        \label{eq:mdp-optimal-value-function-3}
    \end{split}
 \end{equation}


